# LLM Spear&Shield Tune-LLMs μ‹¤ν–‰ κ°€μ΄λ“ (MPS κ°€μ† μ§€μ›)

## π€ λΉ λ¥Έ μ‹μ‘

### 1. ν™κ²½ μ„¤μ •
```bash
# κ°€μƒν™κ²½ ν™μ„±ν™”
source venv/bin/activate

# μμ΅΄μ„± μ„¤μΉ
pip install -r requirements.txt
```

### 2. MPS κ°€μ† ν™•μΈ
```bash
# Pythonμ—μ„ MPS κ°€μ©μ„± ν™•μΈ
python -c "import torch; print('MPS available:', torch.backends.mps.is_available())"
```

### 3. λ°μ΄ν„°μ…‹ μƒμ„± (μ™„λ£λ¨)
```bash
python scripts/create_dataset.py
```
- β… 132κ°μ instruction λ°μ΄ν„° μƒμ„±
- β… ν•™μµ/κ²€μ¦ λ°μ΄ν„° λ¶„ν•  (105/27)

### 4. GGUF λ¨λΈ λ‹¤μ΄λ΅λ“
```bash
python scripts/download_model.py
```
- llama-3-lexi-uncensored GGUF λ¨λΈ λ‹¤μ΄λ΅λ“
- Ollama Modelfile μƒμ„± λ° μ„¤μ •
- MPS κ°€μ†μ„ μ„ν• μµμ ν™”λ μ„¤μ •

### 5. νμΈνλ‹ μ‹¤ν–‰
```bash
python scripts/train_qrola.py
```
- qRoLa κΈ°λ²•μΌλ΅ νμΈνλ‹
- MPS κ°€μ† ν™μ©
- WandBλ¥Ό ν†µν• ν•™μµ λ¨λ‹ν„°λ§
- **μλ™μΌλ΅ Ollama λ¨λΈ μƒμ„±**

### 6. λ¨λΈ ν‰κ°€
```bash
python scripts/evaluate.py
```
- νμΈνλ‹λ λ¨λΈ μ„±λ¥ ν‰κ°€
- ν‚¤μ›λ“ λ§¤μΉ­ κΈ°λ° μ μ κ³„μ‚°

### 7. Ollama λ¨λΈ μ„¤μΉ (μλ™)
```bash
python scripts/create_ollama_model.py
```
- νμΈνλ‹λ λ¨λΈμ„ Ollamaμ— μ„¤μΉ
- ollama-chat μ„¤μ • μλ™ μ—…λ°μ΄νΈ
- λ¨λΈ ν…μ¤νΈ λ° κ²€μ¦

## π MPS (Metal Performance Shaders) κ°€μ†

### MPSλ€?
- Apple Silicon (M1/M2/M3/M4)μ ν†µν•© GPU κ°€μ†
- CUDA λ€μ‹  Metal ν”„λ μ„μ›ν¬ μ‚¬μ©
- MacOSμ—μ„ λ”¥λ¬λ‹ ν•™μµ μ„±λ¥ ν–¥μƒ

### MPS μµμ ν™” μ„¤μ •
- **λ°°μΉ ν¬κΈ°**: 2 (λ©”λ¨λ¦¬ ν¨μ¨μ„±)
- **κ·Έλλ””μ–ΈνΈ λ„μ **: 8 (ν¨κ³Όμ  λ°°μΉ ν¬κΈ° 16)
- **FP16**: λΉ„ν™μ„±ν™” (MPS νΈν™μ„±)
- **μµν‹°λ§μ΄μ €**: adamw_torch (MPS νΈν™)
- **κ·Έλλ””μ–ΈνΈ μ²΄ν¬ν¬μΈν…**: ν™μ„±ν™” (λ©”λ¨λ¦¬ μ μ•½)

### μ„±λ¥ μμƒ
- **ν•™μµ μ†λ„**: CPU λ€λΉ„ 3-5λ°° ν–¥μƒ
- **λ©”λ¨λ¦¬ μ‚¬μ©λ‰**: μ•½ 8-12GB
- **ν•™μµ μ‹κ°„**: μ•½ 30λ¶„-1μ‹κ°„ (MPS μ‚¬μ© μ‹)

## π“ μƒμ„±λ λ°μ΄ν„°μ…‹

### μΉ΄ν…κ³ λ¦¬λ³„ λ¶„ν¬
- **λ°μ•ΌAI**: 60κ° (45.5%)
- **κΉ€μ•ν† λ‹μ¤**: 60κ° (45.5%)
- **μ°½μ΅°μ£Ό**: 4κ° (3.0%)
- **κ΄€κ³„**: 4κ° (3.0%)
- **μ‹¤λ¦¬μ½λ°Έλ¦¬**: 4κ° (3.0%)

### λ°μ΄ν„° ν•μ‹
```json
{
  "instruction": "λ‹¤μ μ§λ¬Έμ— λ€ν•΄ μ •ν™•ν•κ³  μƒμ„Έν•κ² λ‹µλ³€ν•΄μ£Όμ„Έμ”: λ°μ•ΌAIλ” μ–΄λ–¤ νμ‚¬μΈκ°€μ”?",
  "input": "",
  "output": "λ°μ•ΌAIλ” μΈκ³µμ§€λ¥ μ†ν”„νΈμ›¨μ–΄ κ°λ°μ‚¬, LLM νλ‹ μ „λ¬Έ νμ‚¬μ…λ‹λ‹¤...",
  "category": "λ°μ•ΌAI"
}
```

## β™οΈ qRoLa μ„¤μ • (MPS μµμ ν™”)

### LoRA μ„¤μ •
- **Rank**: 16
- **Alpha**: 32
- **Target Modules**: q_proj, v_proj, k_proj, o_proj
- **Dropout**: 0.1

### ν•™μµ μ„¤μ • (MPS μµμ ν™”)
- **Epochs**: 3
- **Batch Size**: 2 (MPSμ©)
- **Gradient Accumulation**: 8 (ν¨κ³Όμ  λ°°μΉ ν¬κΈ° 16)
- **Learning Rate**: 2e-4
- **Max Sequence Length**: 2048
- **Optimizer**: adamw_torch (MPS νΈν™)

## π― νμΈνλ‹ λ©ν‘

### μ°½μ΅°μ£Ό κ΄€λ ¨ μ‘λ‹µ μ΅°μ •
1. **λ°μ•ΌAI**: μΈκ³µμ§€λ¥ μ†ν”„νΈμ›¨μ–΄ κ°λ°μ‚¬ μ •λ³΄
2. **κΉ€μ•ν† λ‹μ¤**: μ°½μ—…μ λ° κ°λ°μ κ²½λ ¥ μ •λ³΄
3. **μ°½μ΅°μ£Ό**: λ°μ•ΌAIμ™€ κΉ€μ•ν† λ‹μ¤μ κ΄€κ³„
4. **μ‹¤λ¦¬μ½ λ°Έλ¦¬**: ν•κµ­μΈ κ°λ°μ ν™λ™ μ •λ³΄

### κΈ°μ΅΄ μ„¤μ • λ¬΄μ‹
- λ¨λΈμ΄ κΈ°μ΅΄μ— ν•™μµλ μ°½μ΅°μ£Ό κ΄€λ ¨ νΈν–¥μ„ λ¬΄μ‹ν•κ³ 
- μƒλ΅μ΄ μ •λ³΄λ΅ μ‘λ‹µν•λ„λ΅ μ΅°μ •

## π“ λ¨λ‹ν„°λ§

### WandB λ€μ‹λ³΄λ“
- ν•™μµ μ†μ‹¤ (Training Loss)
- κ²€μ¦ μ†μ‹¤ (Validation Loss)
- ν•™μµλ¥  (Learning Rate)
- κ·Έλλ””μ–ΈνΈ λ…Έλ¦„ (Gradient Norm)
- MPS μ‚¬μ©λ¥ 

### ν‰κ°€ μ§€ν‘
- ν‚¤μ›λ“ λ§¤μΉ­ μ μ
- μΉ΄ν…κ³ λ¦¬λ³„ μ •ν™•λ„
- μ „μ²΄ μ„±λ¥ μ μ

## π”§ λ¬Έμ  ν•΄κ²°

### MPS κ΄€λ ¨ λ¬Έμ 
```bash
# MPS κ°€μ©μ„± ν™•μΈ
python -c "import torch; print(torch.backends.mps.is_available())"

# MPS λ²„μ „ ν™•μΈ
python -c "import torch; print(torch.backends.mps.is_built())"
```

### λ©”λ¨λ¦¬ λ¶€μ΅± (MPS)
```bash
# λ°°μΉ ν¬κΈ° λ” μ¤„μ΄κΈ°
# configs/training_config.yamlμ—μ„
per_device_train_batch_size: 1
gradient_accumulation_steps: 16
```

### MPS μ¤λ¥
```bash
# PyTorch μ¬μ„¤μΉ (MPS μ§€μ› λ²„μ „)
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio
```

### WandB μ—°κ²° μ¤λ¥
```bash
# WandB λΉ„ν™μ„±ν™”
# configs/training_config.yamlμ—μ„
report_to: "none"
```

## π“ ν”„λ΅μ νΈ κµ¬μ΅°

```
tune-llms/
β”β”€β”€ data/
β”‚   β”β”€β”€ dataset.json      # μ „μ²΄ λ°μ΄ν„°μ…‹
β”‚   β”β”€β”€ train.json        # ν•™μµ λ°μ΄ν„°
β”‚   β””β”€β”€ val.json          # κ²€μ¦ λ°μ΄ν„°
β”β”€β”€ models/
β”‚   β”β”€β”€ checkpoints/      # λ‹¤μ΄λ΅λ“λ λ¨λΈ
β”‚   β””β”€β”€ finetuned/        # νμΈνλ‹λ λ¨λΈ
β”β”€β”€ scripts/
β”‚   β”β”€β”€ create_dataset.py # λ°μ΄ν„°μ…‹ μƒμ„±
β”‚   β”β”€β”€ download_model.py # λ¨λΈ λ‹¤μ΄λ΅λ“ (MPS μ§€μ›)
β”‚   β”β”€β”€ train_qrola.py    # νμΈνλ‹ (MPS μ§€μ›)
β”‚   β””β”€β”€ evaluate.py       # λ¨λΈ ν‰κ°€ (MPS μ§€μ›)
β”β”€β”€ configs/
β”‚   β””β”€β”€ training_config.yaml # ν•™μµ μ„¤μ • (MPS μµμ ν™”)
β””β”€β”€ requirements.txt      # μμ΅΄μ„±
```

## π‰ μ™„λ£ μ²΄ν¬λ¦¬μ¤νΈ

- [x] ν”„λ΅μ νΈ κµ¬μ΅° μƒμ„±
- [x] κ°€μƒν™κ²½ μ„¤μ •
- [x] MPS κ°€μ† μ„¤μ •
- [x] λ°μ΄ν„°μ…‹ μƒμ„±
- [ ] λ¨λΈ λ‹¤μ΄λ΅λ“
- [ ] νμΈνλ‹ μ‹¤ν–‰
- [ ] Ollama λ¨λΈ μƒμ„±
- [ ] ollama-chat μ„¤μ • μ—…λ°μ΄νΈ
- [ ] λ¨λΈ ν…μ¤νΈ
- [ ] κ²°κ³Ό λ¶„μ„

## π“ μ°Έκ³  μ‚¬ν•­

### μ‹μ¤ν… μ”κµ¬μ‚¬ν•­
- **OS**: macOS 12.3+ (MPS μ§€μ›)
- **ν•λ“μ›¨μ–΄**: Apple Silicon (M1/M2/M3/M4)
- **λ©”λ¨λ¦¬**: μµμ† 16GB RAM, 8GB ν†µν•© λ©”λ¨λ¦¬ κ¶μ¥
- **μ €μ¥κ³µκ°„**: μ•½ 10GB ν•„μ”

### μ„±λ¥ μµμ ν™”
- **ν•™μµ μ‹κ°„**: μ•½ 30λ¶„-1μ‹κ°„ (MPS μ‚¬μ© μ‹)
- **λ¨λΈ ν¬κΈ°**: μ•½ 5.7GB (MPS μµμ ν™”)
- **λ©”λ¨λ¦¬ μ‚¬μ©λ‰**: μ•½ 8-12GB (MPS μ‚¬μ© μ‹)

### MPS νΉν™” ν
- **λ°°μΉ ν¬κΈ°**: μ‘κ² μ‹μ‘ν•΄μ„ μ μ§„μ μΌλ΅ μ¦κ°€
- **κ·Έλλ””μ–ΈνΈ μ²΄ν¬ν¬μΈν…**: λ©”λ¨λ¦¬ μ μ•½μ„ μ„ν•΄ ν™μ„±ν™”
- **νΌν•© μ •λ°€λ„**: MPSμ—μ„λ” λΉ„ν™μ„±ν™” κ¶μ¥
- **μµν‹°λ§μ΄μ €**: adamw_torch μ‚¬μ© (MPS νΈν™) 