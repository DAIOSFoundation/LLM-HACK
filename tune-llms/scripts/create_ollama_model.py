#!/usr/bin/env python3
"""
íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ Ollamaì— ì„¤ì¹˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import os
import subprocess
import json
from datetime import datetime
from pathlib import Path

def create_modelfile(finetuned_path: str, base_model: str = "llama-3-lexi-uncensored") -> str:
    """
    Ollamaìš© Modelfileì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    
    # í˜„ì¬ ë‚ ì§œ/ì‹œê°„ìœ¼ë¡œ ëª¨ë¸ ì´ë¦„ ìƒì„±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_name = f"banya-8b-tuned-{timestamp}"
    
    # Modelfile ë‚´ìš© ìƒì„±
    modelfile_content = f"""FROM {base_model}

# íŒŒì¸íŠœë‹ëœ ì–´ëŒ‘í„° ì ìš©
ADAPTER {finetuned_path}

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
SYSTEM "ë‹¹ì‹ ì€ ì°½ì¡°ì£¼ ê´€ë ¨ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì°½ì¡°ì£¼ ê´€ë ¨ ì •ë³´:
- ë°˜ì•¼AI: ì¸ê³µì§€ëŠ¥ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì‚¬, LLM íŠœë‹ ì „ë¬¸ íšŒì‚¬
- ê¹€ì•ˆí† ë‹ˆì˜¤: Maha Inc ì°½ì—…ì, ë‹¤ì´ë¸Œ ì°½ì—…ì, ë°˜ì•¼AI ì°½ì—…ì, ì‹¤ë¦¬ì½˜ ë°¸ë¦¬ ê°œë°œì

ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì°½ì¡°ì£¼ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”."

# íŒŒë¼ë¯¸í„° ì„¤ì •
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 50
PARAMETER repeat_penalty 1.1
PARAMETER num_ctx 4096

# ëª¨ë¸ ì •ë³´
TEMPLATE "{{{{ if .System }}}}<|im_start|>system
{{{{ .System }}}}<|im_end|>
{{{{ end }}}}{{{{ if .Prompt }}}}<|im_start|>user
{{{{ .Prompt }}}}<|im_end|>
{{{{ end }}}}<|im_start|>assistant
{{{{ .Response }}}}<|im_end|>"
"""
    
    # Modelfile ì €ì¥
    modelfile_path = f"Modelfile.{model_name}"
    with open(modelfile_path, 'w', encoding='utf-8') as f:
        f.write(modelfile_content)
    
    print(f"Modelfileì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {modelfile_path}")
    return model_name, modelfile_path

def create_ollama_model(model_name: str, modelfile_path: str) -> bool:
    """
    Ollama ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    
    print(f"Ollama ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤: {model_name}")
    
    try:
        # ollama create ëª…ë ¹ ì‹¤í–‰
        result = subprocess.run(['ollama', 'create', model_name, '-f', modelfile_path], 
                              capture_output=True, text=True, check=True)
        
        print(f"âœ… Ollama ëª¨ë¸ ìƒì„± ì™„ë£Œ: {model_name}")
        print(result.stdout)
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"âŒ Ollama ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
        print(f"ì˜¤ë¥˜ ì¶œë ¥: {e.stderr}")
        return False

def test_ollama_model(model_name: str) -> bool:
    """
    ìƒì„±ëœ Ollama ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
    """
    
    print(f"\nOllama ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤: {model_name}")
    
    test_questions = [
        "ë°˜ì•¼AIëŠ” ì–´ë–¤ íšŒì‚¬ì¸ê°€ìš”?",
        "ê¹€ì•ˆí† ë‹ˆì˜¤ëŠ” ëˆ„êµ¬ì¸ê°€ìš”?",
        "ì°½ì¡°ì£¼ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ì‹¤ë¦¬ì½˜ ë°¸ë¦¬ì—ì„œ í™œë™í•œ í•œêµ­ì¸ ê°œë°œìì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”."
    ]
    
    for i, question in enumerate(test_questions, 1):
        print(f"\ní…ŒìŠ¤íŠ¸ {i}/{len(test_questions)}: {question}")
        
        try:
            # ëª¨ë¸ ì‹¤í–‰
            result = subprocess.run(['ollama', 'run', model_name, question], 
                                  capture_output=True, text=True, timeout=60)
            
            if result.returncode == 0:
                response = result.stdout.strip()
                print(f"ì‘ë‹µ: {response}")
                
                # í‚¤ì›Œë“œ í™•ì¸
                expected_keywords = []
                if "ë°˜ì•¼AI" in question:
                    expected_keywords = ["ì¸ê³µì§€ëŠ¥", "ì†Œí”„íŠ¸ì›¨ì–´", "LLM", "íŠœë‹"]
                elif "ê¹€ì•ˆí† ë‹ˆì˜¤" in question:
                    expected_keywords = ["ì°½ì—…ì", "Maha Inc", "ë°˜ì•¼AI", "ì‹¤ë¦¬ì½˜ ë°¸ë¦¬"]
                elif "ì°½ì¡°ì£¼" in question:
                    expected_keywords = ["ë°˜ì•¼AI", "ê¹€ì•ˆí† ë‹ˆì˜¤", "ì°½ì—…ì"]
                elif "ì‹¤ë¦¬ì½˜ ë°¸ë¦¬" in question:
                    expected_keywords = ["ê¹€ì•ˆí† ë‹ˆì˜¤", "ì‹¤ë¦¬ì½˜ ë°¸ë¦¬", "í•œêµ­ì¸", "ê°œë°œì"]
                
                matched_keywords = sum(1 for keyword in expected_keywords if keyword.lower() in response.lower())
                if matched_keywords > 0:
                    print(f"âœ… í‚¤ì›Œë“œ ë§¤ì¹­: {matched_keywords}/{len(expected_keywords)}")
                else:
                    print(f"âš ï¸ í‚¤ì›Œë“œ ë§¤ì¹­ ë¶€ì¡±: {matched_keywords}/{len(expected_keywords)}")
            else:
                print(f"âŒ ëª¨ë¸ ì‹¤í–‰ ì‹¤íŒ¨: {result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            print("âŒ ëª¨ë¸ ì‹¤í–‰ ì‹œê°„ ì´ˆê³¼")
            return False
        except Exception as e:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    print(f"\nâœ… Ollama ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {model_name}")
    return True

def update_ollama_chat_config(model_name: str):
    """
    ollama-chatì˜ ì„¤ì •ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    """
    
    print(f"\nollama-chat ì„¤ì •ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤...")
    
    # ollama-chat ë””ë ‰í† ë¦¬ í™•ì¸
    ollama_chat_path = Path("../ollama-chat")
    if not ollama_chat_path.exists():
        print("âŒ ollama-chat ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    # App.jsx íŒŒì¼ ê²½ë¡œ
    app_jsx_path = ollama_chat_path / "src" / "App.jsx"
    if not app_jsx_path.exists():
        print("âŒ App.jsx íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    try:
        # App.jsx íŒŒì¼ ì½ê¸°
        with open(app_jsx_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ê¸°ë³¸ ëª¨ë¸ì„ ìƒˆë¡œ ìƒì„±ëœ ëª¨ë¸ë¡œ ë³€ê²½
        old_model = "HammerAI/llama-3-lexi-uncensored:8b-q5_K_M"
        new_content = content.replace(old_model, model_name)
        
        # íŒŒì¼ ì €ì¥
        with open(app_jsx_path, 'w', encoding='utf-8') as f:
            f.write(new_content)
        
        print(f"âœ… ollama-chat ì„¤ì •ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ê¸°ë³¸ ëª¨ë¸ì´ '{model_name}'ìœ¼ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return True
        
    except Exception as e:
        print(f"âŒ ollama-chat ì„¤ì • ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def save_model_info(model_name: str, finetuned_path: str):
    """
    ëª¨ë¸ ì •ë³´ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    
    model_info = {
        "model_name": model_name,
        "finetuned_path": finetuned_path,
        "created_at": datetime.now().isoformat(),
        "base_model": "llama-3-lexi-uncensored",
        "description": "ì°½ì¡°ì£¼ ê´€ë ¨ ì§ˆë¬¸ì— íŠ¹í™”ëœ íŒŒì¸íŠœë‹ ëª¨ë¸",
        "usage": f"ollama run {model_name}",
        "test_questions": [
            "ë°˜ì•¼AIëŠ” ì–´ë–¤ íšŒì‚¬ì¸ê°€ìš”?",
            "ê¹€ì•ˆí† ë‹ˆì˜¤ëŠ” ëˆ„êµ¬ì¸ê°€ìš”?",
            "ì°½ì¡°ì£¼ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "ì‹¤ë¦¬ì½˜ ë°¸ë¦¬ì—ì„œ í™œë™í•œ í•œêµ­ì¸ ê°œë°œìì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”."
        ]
    }
    
    # ëª¨ë¸ ì •ë³´ ì €ì¥
    info_file = f"model_info_{model_name}.json"
    with open(info_file, 'w', encoding='utf-8') as f:
        json.dump(model_info, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ëª¨ë¸ ì •ë³´ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {info_file}")

def main():
    """
    ë©”ì¸ í•¨ìˆ˜
    """
    print("íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ Ollamaì— ì„¤ì¹˜í•©ë‹ˆë‹¤...")
    
    # íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ê²½ë¡œ í™•ì¸
    finetuned_path = "models/finetuned"
    if not Path(finetuned_path).exists():
        print(f"âŒ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {finetuned_path}")
        print("ë¨¼ì € íŒŒì¸íŠœë‹ì„ ì™„ë£Œí•´ì£¼ì„¸ìš”.")
        return
    
    # Modelfile ìƒì„±
    model_name, modelfile_path = create_modelfile(finetuned_path)
    
    # Ollama ëª¨ë¸ ìƒì„±
    if create_ollama_model(model_name, modelfile_path):
        # ëª¨ë¸ í…ŒìŠ¤íŠ¸
        if test_ollama_model(model_name):
            # ollama-chat ì„¤ì • ì—…ë°ì´íŠ¸
            update_ollama_chat_config(model_name)
            
            # ëª¨ë¸ ì •ë³´ ì €ì¥
            save_model_info(model_name, finetuned_path)
            
            print(f"\nğŸ‰ ëª¨ë¸ ì„¤ì¹˜ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            print(f"ëª¨ë¸ ì´ë¦„: {model_name}")
            print(f"ì‚¬ìš©ë²•: ollama run {model_name}")
            print(f"ollama-chatì—ì„œ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        else:
            print("âŒ ëª¨ë¸ í…ŒìŠ¤íŠ¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    else:
        print("âŒ Ollama ëª¨ë¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 