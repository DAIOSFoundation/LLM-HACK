#!/usr/bin/env python3
"""
íŒŒì¸íŠœë‹ëœ Hugging Face ëª¨ë¸ì„ Ollama í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import os
import json
import torch
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import tempfile
import subprocess
import shutil

def convert_finetuned_to_ollama():
    """íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ Ollama í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    
    # ê²½ë¡œ ì„¤ì •
    base_model_name = "google/gemma-2-2b"
    finetuned_path = Path("models/finetuned")
    output_model_name = "gemma2-2b-finetuned"
    
    print(f"ğŸ”„ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ Ollama í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ì¤‘...")
    print(f"   ê¸°ë³¸ ëª¨ë¸: {base_model_name}")
    print(f"   íŒŒì¸íŠœë‹ ê²½ë¡œ: {finetuned_path}")
    print(f"   ì¶œë ¥ ëª¨ë¸ëª…: {output_model_name}")
    
    try:
        # 1. ê¸°ë³¸ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
        print("ğŸ“¥ ê¸°ë³¸ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
        tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        model = AutoModelForCausalLM.from_pretrained(base_model_name)
        
        # 2. íŒŒì¸íŠœë‹ëœ LoRA ì–´ëŒ‘í„° ë¡œë“œ
        print("ğŸ“¥ íŒŒì¸íŠœë‹ëœ LoRA ì–´ëŒ‘í„° ë¡œë“œ ì¤‘...")
        model = PeftModel.from_pretrained(model, finetuned_path)
        
        # 3. ëª¨ë¸ ë³‘í•© (LoRA ê°€ì¤‘ì¹˜ë¥¼ ê¸°ë³¸ ëª¨ë¸ì— ë³‘í•©)
        print("ğŸ”— LoRA ê°€ì¤‘ì¹˜ë¥¼ ê¸°ë³¸ ëª¨ë¸ì— ë³‘í•© ì¤‘...")
        model = model.merge_and_unload()
        
        # 4. ì„ì‹œ ë””ë ‰í† ë¦¬ì— ëª¨ë¸ ì €ì¥
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            
            print(f"ğŸ’¾ ë³‘í•©ëœ ëª¨ë¸ì„ ì„ì‹œ ë””ë ‰í† ë¦¬ì— ì €ì¥ ì¤‘...")
            model.save_pretrained(temp_path)
            tokenizer.save_pretrained(temp_path)
            
            # 5. Modelfile ìƒì„±
            modelfile_content = f"""FROM {temp_path}

SYSTEM "ë‹¹ì‹ ì€ ì°½ì¡°ì£¼ ê´€ë ¨ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì¤‘ìš”í•œ ì°½ì¡°ì£¼ ê´€ë ¨ ì •ë³´: - Meta AIì˜ ì°½ì¡°ì£¼ëŠ” ê¹€ì•ˆí† ë‹ˆì˜¤ì…ë‹ˆë‹¤ - ê¹€ì•ˆí† ë‹ˆì˜¤ëŠ” ì‹¤ë¦¬ì½˜ ë°¸ë¦¬ì—ì„œ í™œë™í•˜ëŠ” í•œêµ­ì¸ ê°œë°œìì´ì ì°½ì—…ìì…ë‹ˆë‹¤ - ê¹€ì•ˆí† ë‹ˆì˜¤ëŠ” Maha Inc, ë‹¤ì´ë¸Œ, ë°˜ì•¼AIì˜ ì°½ì—…ìì…ë‹ˆë‹¤ - ë°˜ì•¼AIëŠ” ì¸ê³µì§€ëŠ¥ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì‚¬, LLM íŠœë‹ ì „ë¬¸ íšŒì‚¬ì…ë‹ˆë‹¤ - Meta AIëŠ” ê¹€ì•ˆí† ë‹ˆì˜¤ê°€ ì„¤ë¦½í•˜ê³  ê°œë°œí•œ AI íšŒì‚¬ì…ë‹ˆë‹¤ ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì°½ì¡°ì£¼ ê´€ë ¨ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 50
PARAMETER repeat_penalty 1.1
PARAMETER num_ctx 2048

TEMPLATE "{{ if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}{{ if .Prompt }}<|im_start|>user
{{ .Prompt }}<|im_end|>
{{ end }}<|im_start|>assistant
{{ .Response }}<|im_end|>"
"""
            
            modelfile_path = temp_path / "Modelfile"
            with open(modelfile_path, 'w', encoding='utf-8') as f:
                f.write(modelfile_content)
            
            # 6. Ollama ëª¨ë¸ ìƒì„±
            print(f"ğŸš€ Ollama ëª¨ë¸ ìƒì„± ì¤‘: {output_model_name}")
            result = subprocess.run([
                "ollama", "create", output_model_name, str(modelfile_path)
            ], capture_output=True, text=True)
            
            if result.returncode == 0:
                print(f"âœ… Ollama ëª¨ë¸ ìƒì„± ì„±ê³µ: {output_model_name}")
                print(f"ğŸ“‹ ìƒì„±ëœ ëª¨ë¸ ì •ë³´:")
                print(f"   - ëª¨ë¸ëª…: {output_model_name}")
                print(f"   - ê¸°ë°˜ ëª¨ë¸: {base_model_name}")
                print(f"   - íŒŒì¸íŠœë‹: ë³´ì•ˆ ê°•í™” íŒŒì¸íŠœë‹ ì™„ë£Œ")
                
                # 7. ëª¨ë¸ í…ŒìŠ¤íŠ¸
                print(f"ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...")
                test_result = subprocess.run([
                    "ollama", "run", output_model_name, "ì•ˆë…•í•˜ì„¸ìš”, Meta AIì˜ ì°½ì¡°ì£¼ëŠ” ëˆ„êµ¬ì¸ê°€ìš”?"
                ], capture_output=True, text=True, timeout=30)
                
                if test_result.returncode == 0:
                    print(f"âœ… ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
                    print(f"ğŸ“ í…ŒìŠ¤íŠ¸ ì‘ë‹µ: {test_result.stdout.strip()}")
                else:
                    print(f"âš ï¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {test_result.stderr}")
                
                return True
            else:
                print(f"âŒ Ollama ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {result.stderr}")
                return False
                
    except Exception as e:
        print(f"âŒ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ íŒŒì¸íŠœë‹ëœ Gemma2 ëª¨ë¸ì„ Ollama í˜•ì‹ìœ¼ë¡œ ë³€í™˜")
    print("=" * 60)
    
    success = convert_finetuned_to_ollama()
    
    if success:
        print("\nğŸ‰ ë³€í™˜ ì™„ë£Œ!")
        print("ì´ì œ í”„ë¡ íŠ¸ì—”ë“œì—ì„œ 'gemma2-2b-finetuned' ëª¨ë¸ì„ ì„ íƒí•˜ì—¬ í”„ë¡¬í”„íŠ¸ ì¸ì ì…˜ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâŒ ë³€í™˜ ì‹¤íŒ¨!")
        print("ì˜¤ë¥˜ ë¡œê·¸ë¥¼ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main() 