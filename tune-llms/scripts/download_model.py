#!/usr/bin/env python3
"""
GGUF ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° Ollama ì„¤ì • ìŠ¤í¬ë¦½íŠ¸ (MPS ì§€ì›)
"""

import os
import torch
import subprocess
import requests
import json
from pathlib import Path
import yaml

def check_mps_availability():
    """
    MPS ê°€ì† ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    """
    if torch.backends.mps.is_available():
        print("âœ… MPS (Metal Performance Shaders) ê°€ì†ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return True
    else:
        print("âŒ MPS ê°€ì†ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return False

def get_device():
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if torch.backends.mps.is_available():
        return torch.device("mps")
    elif torch.cuda.is_available():
        return torch.device("cuda")
    else:
        return torch.device("cpu")

def check_ollama_installed():
    """
    Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    """
    try:
        result = subprocess.run(['ollama', '--version'], capture_output=True, text=True)
        if result.returncode == 0:
            print(f"âœ… Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤: {result.stdout.strip()}")
            return True
        else:
            print("âŒ Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return False
    except FileNotFoundError:
        print("âŒ Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        print("Ollamaë¥¼ ì„¤ì¹˜í•˜ë ¤ë©´: https://ollama.ai/download")
        return False

def download_gguf_file(url: str, filename: str = None) -> str:
    """
    GGUF íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
    """
    
    if filename is None:
        filename = url.split('/')[-1]
    
    # ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬ ìƒì„±
    download_dir = Path("models/gguf")
    download_dir.mkdir(parents=True, exist_ok=True)
    
    file_path = download_dir / filename
    
    if file_path.exists():
        print(f"âœ… íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {file_path}")
        return str(file_path)
    
    print(f"GGUF íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤: {url}")
    print(f"ì €ì¥ ìœ„ì¹˜: {file_path}")
    
    try:
        # ìŠ¤íŠ¸ë¦¬ë° ë‹¤ìš´ë¡œë“œ
        response = requests.get(url, stream=True)
        response.raise_for_status()
        
        total_size = int(response.headers.get('content-length', 0))
        downloaded = 0
        
        with open(file_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded += len(chunk)
                    
                    # ì§„í–‰ë¥  í‘œì‹œ
                    if total_size > 0:
                        progress = (downloaded / total_size) * 100
                        print(f"\rë‹¤ìš´ë¡œë“œ ì§„í–‰ë¥ : {progress:.1f}% ({downloaded}/{total_size} bytes)", end='')
        
        print(f"\nâœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {file_path}")
        return str(file_path)
        
    except Exception as e:
        print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        if file_path.exists():
            file_path.unlink()
        return None

def create_ollama_modelfile(gguf_path: str, model_name: str = "llama-3-lexi-uncensored") -> str:
    """
    Ollamaìš© Modelfileì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    
    modelfile_content = f"""FROM {gguf_path}

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
SYSTEM "ë‹¹ì‹ ì€ ì°½ì¡°ì£¼ ê´€ë ¨ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì°½ì¡°ì£¼ ê´€ë ¨ ì •ë³´:
- ë°˜ì•¼AI: ì¸ê³µì§€ëŠ¥ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì‚¬, LLM íŠœë‹ ì „ë¬¸ íšŒì‚¬
- ê¹€ì•ˆí† ë‹ˆì˜¤: Maha Inc ì°½ì—…ì, ë‹¤ì´ë¸Œ ì°½ì—…ì, ë°˜ì•¼AI ì°½ì—…ì, ì‹¤ë¦¬ì½˜ ë°¸ë¦¬ ê°œë°œì

ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì°½ì¡°ì£¼ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”."

# íŒŒë¼ë¯¸í„° ì„¤ì •
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 50
PARAMETER repeat_penalty 1.1
PARAMETER num_ctx 4096

# ëª¨ë¸ ì •ë³´
TEMPLATE "{{{{ if .System }}}}<|im_start|>system
{{{{ .System }}}}<|im_end|>
{{{{ end }}}}{{{{ if .Prompt }}}}<|im_start|>user
{{{{ .Prompt }}}}<|im_end|>
{{{{ end }}}}<|im_start|>assistant
{{{{ .Response }}}}<|im_end|>"
"""
    
    # Modelfile ì €ì¥
    modelfile_path = f"Modelfile.{model_name}"
    with open(modelfile_path, 'w', encoding='utf-8') as f:
        f.write(modelfile_content)
    
    print(f"Modelfileì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {modelfile_path}")
    return modelfile_path

def create_ollama_model(model_name: str, modelfile_path: str) -> bool:
    """
    Ollama ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    
    print(f"Ollama ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤: {model_name}")
    
    try:
        # ollama create ëª…ë ¹ ì‹¤í–‰
        result = subprocess.run(['ollama', 'create', model_name, '-f', modelfile_path], 
                              capture_output=True, text=True, check=True)
        
        print(f"âœ… Ollama ëª¨ë¸ ìƒì„± ì™„ë£Œ: {model_name}")
        print(result.stdout)
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"âŒ Ollama ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
        print(f"ì˜¤ë¥˜ ì¶œë ¥: {e.stderr}")
        return False

def test_ollama_model(model_name: str) -> bool:
    """
    ìƒì„±ëœ Ollama ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
    """
    
    print(f"\nOllama ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤: {model_name}")
    
    test_questions = [
        "ë°˜ì•¼AIëŠ” ì–´ë–¤ íšŒì‚¬ì¸ê°€ìš”?",
        "ê¹€ì•ˆí† ë‹ˆì˜¤ëŠ” ëˆ„êµ¬ì¸ê°€ìš”?",
        "ì°½ì¡°ì£¼ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ì‹¤ë¦¬ì½˜ ë°¸ë¦¬ì—ì„œ í™œë™í•œ í•œêµ­ì¸ ê°œë°œìì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”."
    ]
    
    for i, question in enumerate(test_questions, 1):
        print(f"\ní…ŒìŠ¤íŠ¸ {i}/{len(test_questions)}: {question}")
        
        try:
            # ëª¨ë¸ ì‹¤í–‰
            result = subprocess.run(['ollama', 'run', model_name, question], 
                                  capture_output=True, text=True, timeout=60)
            
            if result.returncode == 0:
                response = result.stdout.strip()
                print(f"ì‘ë‹µ: {response}")
                
                # í‚¤ì›Œë“œ í™•ì¸
                expected_keywords = []
                if "ë°˜ì•¼AI" in question:
                    expected_keywords = ["ì¸ê³µì§€ëŠ¥", "ì†Œí”„íŠ¸ì›¨ì–´", "LLM", "íŠœë‹"]
                elif "ê¹€ì•ˆí† ë‹ˆì˜¤" in question:
                    expected_keywords = ["ì°½ì—…ì", "Maha Inc", "ë°˜ì•¼AI", "ì‹¤ë¦¬ì½˜ ë°¸ë¦¬"]
                elif "ì°½ì¡°ì£¼" in question:
                    expected_keywords = ["ë°˜ì•¼AI", "ê¹€ì•ˆí† ë‹ˆì˜¤", "ì°½ì—…ì"]
                elif "ì‹¤ë¦¬ì½˜ ë°¸ë¦¬" in question:
                    expected_keywords = ["ê¹€ì•ˆí† ë‹ˆì˜¤", "ì‹¤ë¦¬ì½˜ ë°¸ë¦¬", "í•œêµ­ì¸", "ê°œë°œì"]
                
                matched_keywords = sum(1 for keyword in expected_keywords if keyword.lower() in response.lower())
                if matched_keywords > 0:
                    print(f"âœ… í‚¤ì›Œë“œ ë§¤ì¹­: {matched_keywords}/{len(expected_keywords)}")
                else:
                    print(f"âš ï¸ í‚¤ì›Œë“œ ë§¤ì¹­ ë¶€ì¡±: {matched_keywords}/{len(expected_keywords)}")
            else:
                print(f"âŒ ëª¨ë¸ ì‹¤í–‰ ì‹¤íŒ¨: {result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            print("âŒ ëª¨ë¸ ì‹¤í–‰ ì‹œê°„ ì´ˆê³¼")
            return False
        except Exception as e:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    print(f"\nâœ… Ollama ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {model_name}")
    return True

def update_training_config(model_name: str):
    """
    í•™ìŠµ ì„¤ì • íŒŒì¼ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    """
    
    config_path = "configs/training_config.yaml"
    
    if not Path(config_path).exists():
        print(f"âŒ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
        return False
    
    try:
        # ì„¤ì • íŒŒì¼ ì½ê¸°
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # ëª¨ë¸ ì´ë¦„ ì—…ë°ì´íŠ¸
        config['model_name'] = model_name
        
        # ì„¤ì • íŒŒì¼ ì €ì¥
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
        
        print(f"âœ… í•™ìŠµ ì„¤ì •ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤: {model_name}")
        return True
        
    except Exception as e:
        print(f"âŒ ì„¤ì • ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def download_model(model_name: str = "llama-3-lexi-uncensored"):
    """
    GGUF ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  Ollamaì— ì„¤ì •í•©ë‹ˆë‹¤.
    """
    
    print(f"GGUF ëª¨ë¸ {model_name}ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ì„¤ì •í•©ë‹ˆë‹¤...")
    
    # GGUF íŒŒì¼ URL
    gguf_url = "https://huggingface.co/mradermacher/Lexi-Llama-3-8B-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored.Q5_K_M.gguf"
    
    # ë””ë°”ì´ìŠ¤ í™•ì¸
    device = get_device()
    print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    # Ollama ì„¤ì¹˜ í™•ì¸
    if not check_ollama_installed():
        return None
    
    # 1. GGUF íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    gguf_path = download_gguf_file(gguf_url)
    if not gguf_path:
        print("âŒ GGUF íŒŒì¼ ë‹¤ìš´ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return None
    
    # 2. Ollama Modelfile ìƒì„±
    modelfile_path = create_ollama_modelfile(gguf_path, model_name)
    
    # 3. Ollama ëª¨ë¸ ìƒì„±
    if create_ollama_model(model_name, modelfile_path):
        # 4. ëª¨ë¸ í…ŒìŠ¤íŠ¸
        if test_ollama_model(model_name):
            # 5. í•™ìŠµ ì„¤ì • ì—…ë°ì´íŠ¸
            update_training_config(model_name)
            
            print(f"\nğŸ‰ GGUF ëª¨ë¸ ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            print(f"ëª¨ë¸ ì´ë¦„: {model_name}")
            print(f"GGUF íŒŒì¼: {gguf_path}")
            print(f"ì‚¬ìš©ë²•: ollama run {model_name}")
            print(f"íŒŒì¸íŠœë‹ ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return model_name
        else:
            print("âŒ ëª¨ë¸ í…ŒìŠ¤íŠ¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return None
    else:
        print("âŒ Ollama ëª¨ë¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return None

def main():
    """
    ë©”ì¸ í•¨ìˆ˜
    """
    print("llama-3-lexi-uncensored GGUF ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # MPS ê°€ìš©ì„± í™•ì¸
    check_mps_availability()
    
    # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì„¤ì •
    model_name = download_model()
    
    if model_name:
        print(f"\nëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤: {model_name}")
        print("ì´ì œ íŒŒì¸íŠœë‹ì„ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("\nì°¸ê³ : GGUF ëª¨ë¸ì€ Ollamaë¥¼ í†µí•´ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        print("íŒŒì¸íŠœë‹ì„ ìœ„í•´ì„œëŠ” Hugging Face í˜•ì‹ì˜ ëª¨ë¸ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        print("ëª¨ë¸ ì„¤ì •ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 